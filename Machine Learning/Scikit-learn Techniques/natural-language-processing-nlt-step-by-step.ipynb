{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"Created by: Sangwook Cheon\n\nDate: Dec 24, 2018\n\nThis is step-by-step guide to Natural Language Processing (NLP) using scikit-learn, which I created for reference. I added some useful notes along the way to clarify things. This notebook's content is from A-Z Datascience course, and I hope this will be useful to those who want to review materials covered, or anyone who wants to learn the basics of Natural Language Processing (NLP).\n\n## Content:\n### 1. Natural Language Processing step-by-step \n\nThis algorithm will predict if reviews are positive or negative. This will be done by using Bag of Words model. \n\n## Difference between .tsv and .csv\nFor text analysis, we need to use .tsv, as it is Tab Separated Values, which allows commas to be ignored when using each value. If .csv is used, as there are already many commas within sentences, the program will not know where it should separate between columns. Therefore, .tsv should be used.\n\n### 1) Preparing data"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n#can use read_csv but include parameters.\ndataset = pd.read_csv('../input/Restaurant_Reviews.tsv', delimiter = '\\t', quoting = 3) #\\t --> tab\n#by putting quoting = 3, we ignore double quotes\n\nprint(dataset.shape[0])\ndataset.head(10)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a87090a590f1ecf5ecf2285af85f900e90e541ff"},"cell_type":"markdown","source":"### 2) Cleaning Data\nWe need to get rid of all unnecessary words and numbers, and apply stemming (converting different tenses to the same tense). This is necessary because the machine needs only important words to determine its positivity or negativity."},{"metadata":{"trusted":true,"_uuid":"5fa8a2af67c6f11a5ef113b13954fe207fc78756"},"cell_type":"code","source":"#import cleaning library\nimport re\ndataset[\"Review\"][0]\n\n#Let's clean the first entry as an example\n\nreview = re.sub('[^a-zA-Z]', ' ', dataset[\"Review\"][0], )\n#first parameter --> ^ shows that we should not remove the following characters\n#second parameter --> prevents the program from removing all the spaces.\n#third parameter --> what is being processed\nreview","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b4f23e4b8ca9c2e31484078d01580500c689fe1d"},"cell_type":"code","source":"review = review.lower()\n#makes all the letters lowercase\nreview","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ca179c33f17c7d673c28ea9c4f1eb63c8966c06c"},"cell_type":"code","source":"#further processing\nimport nltk #famous NLP library\nnltk.download('stopwords') #download words that are not significant","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fc7fbcb8ff21aaa6597be3faabcb2033dbf8b297"},"cell_type":"code","source":"#spliting the sentence\nreview = review.split()\nreview","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6241407441e2ac9dff4a7146fedbc6729683a358"},"cell_type":"code","source":"#updating the list, and stemming (taking the root of each word)\nfrom nltk.corpus import stopwords #importing the downloaded list\nfrom nltk.stem.porter import PorterStemmer\n\nps = PorterStemmer() \n\nreview = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))] \n# 'english' is specified so that we are only looking at english words.\n#use set, as it has faster searching algorithm than a list. This is useful for long text.\nreview","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"383a1ebcd79e687f0987ab024d2d66f99ce4c385"},"cell_type":"code","source":"#joining the words into a sentence again\nreview = ' '.join(review)\nreview","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f453599fb263a5ded8331b7b4a6f74c0cf7d2395"},"cell_type":"markdown","source":"This changed \"loved\" to \"love\". Now, let's do this for all the sentences. We can use a for loop to go over all the indexes.\n\n\n"},{"metadata":{"trusted":true,"_uuid":"641c23b6a5004f5171ce60ef977181c461ffb7f6"},"cell_type":"code","source":"#can use read_csv but include parameters.\ndataset = pd.read_csv('../input/Restaurant_Reviews.tsv', delimiter = '\\t', quoting = 3) #\\t --> tab\n#by putting quoting = 3, we ignore double quotes\n\ncorpus = []\n# curpus is a common word for collection of texts.\n\nfor i in range(0, dataset.shape[0]):\n    review = re.sub('[^a-zA-Z]', ' ', dataset[\"Review\"][i])\n    review = review.lower()\n    review = review.split()\n    ps = PorterStemmer()\n    review = [ps.stem(word) for word in review if word not in set(stopwords.words('english'))]\n    review = ' '.join(review)\n    corpus.append(review)\n    \npd_corpus = pd.DataFrame(corpus)\nprint(corpus[999])\nprint('')\nprint(dataset.loc[999, \"Review\"])\n#just to check the last item in both of the lists match","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"73fbaffd2b90ef00d86c612c8ac191933c870db5"},"cell_type":"code","source":"#Creating the bag of words model --> will create a sparse matrix (a lot of zeros)\n#create one column for each unique word. In the rows, if the word is not there, it is 0, and it is, it is 1. \n#This allows each word to be an independent variable leading to a dependent variable (Yes or No) classification\n\n#Tokenization --> \nfrom sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer(max_features = 1500) #restrict sparsity by removing irrelevant words\nx = cv.fit_transform(corpus).toarray()\ny = dataset.iloc[:, 1].values\nprint(x.shape)\nprint(x[[1,2,3], :])\nprint(y[1:5])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ecd3e142d434ae89e0f3232b66798746353999f8"},"cell_type":"markdown","source":"### 3) Training the model\nWe can apply the same classification algorithm to text, as now text became an independent variable. If not familiar with classification, please [refer to this kernel](https://www.kaggle.com/sangwookchn/classification-techniques-using-scikit-learn)\n#Most commonly used: Decision Tree, Naive Bayes"},{"metadata":{"trusted":true,"_uuid":"b32165f94fa929d312d07c464ef3ff8986e1dd6c"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)\n\n#feature scaling is not needed as we only have small integers: 0, 1, 2, 3, ---\n\nfrom sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(X_train, Y_train)\n\ny_pred = classifier.predict(X_test)\n\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(Y_test, y_pred)\ncm\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ec8c8c50dc718c73208fc9df0a38af298956de82"},"cell_type":"markdown","source":"As we only have 800 results to train on, this result is pretty good. We made (55 + 91) = 146 correct guesses and 54 incorrect ones. Now, let's try predicting on arbitrary text."},{"metadata":{"trusted":true,"_uuid":"937ae5723437b939e030bec88fbb48ae4738cce2"},"cell_type":"code","source":"y1 = \"not bad at all\"\ny2 = \"if you think it's bad, you're bad\"\ny3 = \"just wanted to vomit after eating this\"\ny4 = \"it is incredibly bitter and therefore terrible\"\n\nlist1 = [y1, y2, y3, y4]\ncorpus2 = []\n\nfor i in range(0, len(list1)):\n    review = re.sub('[^a-zA-Z]', ' ', list1[i])\n    review = review.lower()\n    review = review.split()\n    ps = PorterStemmer()\n    review = [ps.stem(word) for word in review if word not in set(stopwords.words('english'))]\n    review = ' '.join(review)\n    corpus2.append(review)\n\nx2 = cv.transform(corpus2).toarray()\n\nprint(x2)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7640f2f01dfca0b57f11467af6eb268deafb20b5"},"cell_type":"markdown","source":"The pretrained CountVectorizer is used, as the classifier is trained on this dataset. In the second sentence, as \"bad\" appears twice, the cell corresponding to this sentence and the word \"bad\" will show up as 2."},{"metadata":{"trusted":true,"_uuid":"501ef9647a01de2e1f8bfa85f2f3e8edee8e2709"},"cell_type":"code","source":"y_pred2 = classifier.predict(x2)\ny_pred2","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"437addf2902670a5fc41b388b2176e76d52acb5c"},"cell_type":"markdown","source":"Hm, the first and second sentence were positive reviews... Let's test some simple ones then."},{"metadata":{"trusted":true,"_uuid":"7b4d763157a6af6bea23aad6ab041bfc13b56d60"},"cell_type":"code","source":"y1 = \"it's bad\"\ny2 = \"it's unbelievably bad.\"\ny3 = \"tastes so good\"\ny4 = \"I cannot forget this enjoyable taste\"\ny5 = 'terrible. not tasty at all'\n\nlist1 = [y1, y2, y3, y4, y5]\ncorpus2 = []\n\nfor i in range(0, len(list1)):\n    review = re.sub('[^a-zA-Z]', ' ', list1[i])\n    review = review.lower()\n    review = review.split()\n    ps = PorterStemmer()\n    review = [ps.stem(word) for word in review if word not in set(stopwords.words('english'))]\n    review = ' '.join(review)\n    corpus2.append(review)\n\nx2 = cv.transform(corpus2).toarray()\n\nprint(x2)\n\ny_pred2 = classifier.predict(x2)\ny_pred2\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"81358feda08ee934ccb5996e20741a532057beae"},"cell_type":"markdown","source":"It seems clear that the algorithm confuses with words that may seem positive on its own, such as \"incredible\" and \"unbelievable\". This means the algorithm wasn't able to catch advanced English styles. Possibly, training on larger dataset will work better. Also, I need to research more advanced NLP algorithms out there. I will write another kernel once I find this. "},{"metadata":{"trusted":true,"_uuid":"ce2c1f70fc311c91d6c539f2b9bd692ccbc8ee17"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}
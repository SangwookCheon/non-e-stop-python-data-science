{"cells":[{"metadata":{"_uuid":"d2786e3298f20b87efff0f8574f969b253ebe034"},"cell_type":"markdown","source":"Created by: Sangwook Cheon\n\nDate: Dec 24, 2018\n\nThis is step-by-step guide to Data Preprocessing using Pandas, which I created for reference. I added some useful notes along the way to clarify things. \nThis notebook's content is from A-Z Datascience course, and I hope this will be useful to those who want to review materials covered, or anyone who wants to learn the basics of Data Preprocessing."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt # Plot charts","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# In PyCharm, set the correct directory as the current working directory\n# Using Data.csv\n\ndataset = pd.read_csv('../input/Data.csv')\nx = dataset.iloc[:, :-1].values #all the columns except the last one\ny = dataset.iloc[:, 3].values #Using the Fourth column which contains dependent variables\n\n# x[:,1:3] = x[:, 1:3].astype('float32')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0b9d1f91daa0b324d2fd64ec59118423757e8bb5"},"cell_type":"markdown","source":"**Now, let's prepare the data**"},{"metadata":{"trusted":true,"_uuid":"ab11c971215f536928b3134574d1ab003a922bfb"},"cell_type":"code","source":"#Missing Data\n#Most common way to deal with missing data: take the mean of columns, \n#and replace each missing data with respective mean of the value of that column\n\n#Due to sklearn update, different syntax is used that is different from videos\nfrom sklearn.impute import SimpleImputer\n\nimp = SimpleImputer(missing_values = np.nan, strategy = 'mean') # --> Use np.nan to look for blank values\n#if axis = 0: along columns, if axis = 1: along rows\n#verbose: how much I want to see regarding the running process of the program\nimp = imp.fit(x[:, 1:3])\nx[:, 1:3] = imp.transform(x[:, 1:3])\n\nx = pd.DataFrame(x) #Converts np array to a pd DataFrame\nx.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"68e45b909c0489298c5e57ee04e4f2ec375c0768"},"cell_type":"markdown","source":"**Categorical Data** contains categories, such as Country names, Yes or No,  which are different from numeric values. As machine learning models are based on mathematics, categorical data needs to be encoded into numbers"},{"metadata":{"trusted":true,"_uuid":"b1b819020ca83fc8ac8f1a8e4a0faefe079d0385"},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nlabelencoder_x = LabelEncoder()\nx.iloc[:, 0] = labelencoder_x.fit_transform(x.iloc[:, 0])\nx.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ceb69464cb902a51229628b0e90ae6d87ce1654f"},"cell_type":"markdown","source":"However, this makes machine learning models assume 2 > 1 > 0, which may affect the result, while categories cannot be compared. therefore, **Dummy encoding** should be used. This is also called **One Hot Encoding**. The category should still be labeled as numbers first, then encoded as OneHot."},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"7bff5d8fa9742c01ce5c1f771cf79b9db9f4e9c5"},"cell_type":"code","source":"from keras.utils import to_categorical #for onehot encoding --> use keras as it is easier. Note that sklearn and keras are great to be used together\n\nencoded = pd.DataFrame(to_categorical(x.iloc[:, 0]))\npd.concat([encoded, x], axis = 1) #along rows, as they have same index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3c5a0050f863c1734a6c74aa89bd18837d2a209b"},"cell_type":"code","source":"#Yes and No should be converted just to numbers, not OneHotEncoding\n\nlabelencoder_y = LabelEncoder()\ny = labelencoder_y.fit_transform(y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"42064101dd4c68f7d09da68bfb9bcfcec7ccd017"},"cell_type":"markdown","source":"Now, let's divide the data set: **Training and Test set**"},{"metadata":{"trusted":true,"_uuid":"29d2ff5c525942700876ae621b018b495545a97d"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)\n#random state enables random sampling. Can put any number, but 42 is recommended","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e9d013612af00e7c0d7eab6fe28652f1728168cd"},"cell_type":"markdown","source":"**Feature Scaling** When two variables are very different in size, the bigger one dominates the machine learning model. This is why variables should be at the same scale. Strategies:\n![182](https://i.imgur.com/r3i7RXH.png)","attachments":{}},{"metadata":{"trusted":true,"_uuid":"f19409d4856f985bde7e4c0985bbed27a4ab4cdb"},"cell_type":"code","source":"#Feature scaling\n\nfrom sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.transform(X_test) \n#As sc_X already fitted to train set, it should only transform the X_test by applying its fit\n\n#For Y_test and Y_train, no feature scaling as it is simple categories\n\n#OneHotCoded values may or may not be scaled, but it depends on context","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}
